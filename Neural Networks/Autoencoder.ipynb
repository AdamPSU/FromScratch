{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:10:08.215952Z","iopub.execute_input":"2024-11-12T17:10:08.217143Z","iopub.status.idle":"2024-11-12T17:10:14.767588Z","shell.execute_reply.started":"2024-11-12T17:10:08.217089Z","shell.execute_reply":"2024-11-12T17:10:14.766471Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def load_data():\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Lambda(lambda x: x.view(-1))])\n\n    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    x_train = train_dataset.data.numpy().reshape(-1, 784) / 255.0\n    x_test = test_dataset.data.numpy().reshape(-1, 784) / 255.0\n    \n    return x_train, x_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:11:24.379262Z","iopub.execute_input":"2024-11-12T17:11:24.379863Z","iopub.status.idle":"2024-11-12T17:11:24.387499Z","shell.execute_reply.started":"2024-11-12T17:11:24.379821Z","shell.execute_reply":"2024-11-12T17:11:24.386228Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def relu(x):\n    \"\"\"Applies the ReLU activation function element-wise.\n\n    Parameters:\n        x (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Array with ReLU applied, where each element is the maximum \n                    of 0 and the input element.\n    \"\"\"\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    \"\"\"Applies the Sigmoid activation function element-wise.\n\n    Parameters:\n        x (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Array with Sigmoid applied to each element.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef lin_combination(x, weights, bias):\n    \"\"\"Computes a linear combination of inputs, weights, and bias.\n\n    Parameters:\n        x (np.ndarray): Input array.\n        weights (np.ndarray): Weight matrix.\n        bias (np.ndarray): Bias vector.\n\n    Returns:\n        np.ndarray: Result of the linear combination.\n    \"\"\"\n    return np.dot(x, weights) + bias\n\ndef mse_loss(x, x_reconstructed):\n    return np.mean((x - x_reconstructed)**2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:51:01.422590Z","iopub.execute_input":"2024-11-12T17:51:01.423026Z","iopub.status.idle":"2024-11-12T17:51:01.470545Z","shell.execute_reply.started":"2024-11-12T17:51:01.422983Z","shell.execute_reply":"2024-11-12T17:51:01.469066Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class AutoEncoder:\n    def __init__(self, epochs=100, lr=0.001):\n        # Initialize weights and biases\n        \n        self.w1 = np.random.randn(784, 512) * 0.01\n        self.b1 = np.zeros(512)\n        \n        self.w2 = np.random.randn(512, 256) * 0.01 \n        self.b2 = np.zeros(256) \n        \n        self.w3 = np.random.randn(256, 128) * 0.01\n        self.b3 = np.zeros(128) \n        \n        self.w4 = np.random.randn(128, 256) * 0.01\n        self.b4 = np.zeros(256)\n        \n        self.w5 = np.random.randn(256, 512) * 0.01\n        self.b5 = np.zeros(512)\n        \n        self.w6 = np.random.randn(512, 784) * 0.01\n        self.b6 = np.zeros(784)\n\n    def forward(self, X): \n        # Encoder: multiple layers with ReLU activation\n        self.layer_1 = relu(lin_combination(x, w1, b1))\n        self.layer_2 = relu(lin_combination(self.layer_1, w2, b2))\n        self.layer_3 = relu(lin_combination(self.layer_2, w3, b3)) # Encoded\n    \n        # Decoder: multiple layers with ReLU activation for reconstruction\n        self.layer_4 = relu(lin_combination(self.layer_3, w4, b4)) \n        self.layer_5 = relu(lin_combination(self.layer_4, w5, b5))\n        self.layer_6 = relu(lin_combination(self.layer_5, w6, b6)) # Decoded\n        \n        return self.layer_3.T, self.layer_6.T\n\n    def backward(self):\n        pass\n\n    def train(self, X, epochs, batch_size, lr):\n        # Initialize lists for tracking metrics\n        losses = []\n        latent_codes = []\n        reconstructed_data = []\n        \n        start_time = time.time()  # Start measuring time\n    \n        num_samples = X.shape[0]  # Define number of samples in dataset\n    \n        for epoch in range(epochs):\n            # Shuffle data at the start of each epoch\n            indices = np.random.permutation(num_samples)\n            X_shuffled = X[indices]\n            epoch_loss = 0\n    \n            # Divide data into mini-batches\n            for i in range(0, num_samples, batch_size):\n                X_batch = X_shuffled[i:i + batch_size] #  Extract the mini-batch\n                output = self.forward(X_batch) #  Forward pass to get reconstructed output\n\n                # Calculate loss (mean squared error between input and reconstruction)\n                loss = mse_loss(X_batch, output)\n                epoch_loss += loss \n                \n                # Backpropagation\n                self.backward(X_batch, X_batch, output, lr)\n    \n            # Calculate and store the average loss for this epoch\n            avg_loss = epoch_loss / (num_samples / batch_size)\n            losses.append(avg_loss)\n            \n            # Optional: Collect latent codes and reconstructed data at intervals (e.g., every 10 epochs)\n            if epoch % 10 == 0:\n                latent_codes.append(self.get_latent(X))  # Placeholder for function to get latent codes\n                reconstructed_data.append(self.forward(X))  # Forward pass on full dataset for reconstruction\n    \n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n        \n        print(\"Training complete in {:.2f} seconds.\".format(time.time() - start_time))\n\n    return losses, latent_codes, reconstructed_data\n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 8. Plot the loss curve\ndef plot_loss_curve(losses):\n    plt.figure(figsize=(8, 6))\n    plt.plot(losses, label='MSE Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Curve')\n    plt.legend()\n    plt.show()\n\n# Uncomment after training to plot the loss curve\n# plot_loss_curve(losses)\n\n# 9. Visualize latent codes using PCA\ndef visualize_latent_space(latent_codes):\n    pca = PCA(n_components=2)\n    reduced_codes = pca.fit_transform(latent_codes)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(reduced_codes[:, 0], reduced_codes[:, 1], s=5, alpha=0.6)\n    plt.title('Latent Space Visualization (PCA)')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.show()\n\n# Uncomment after training to visualize latent space\n# visualize_latent_space(latent_codes)\n\n# 10. Show original and reconstructed images side by side\ndef show_images(original, reconstructed, n=10):\n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n    plt.show()\n\n# Uncomment after training to show images\n# show_images(x_train, reconstructed_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}