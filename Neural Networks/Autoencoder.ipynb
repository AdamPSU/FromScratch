{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-08T04:32:06.343654Z","iopub.execute_input":"2024-11-08T04:32:06.344111Z","iopub.status.idle":"2024-11-08T04:32:12.741725Z","shell.execute_reply.started":"2024-11-08T04:32:06.344041Z","shell.execute_reply":"2024-11-08T04:32:12.740413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_data():\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Lambda(lambda x: x.view(-1))])\n\n    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    x_train = train_dataset.data.numpy().reshape(-1, 784) / 255.0\n    x_test = test_dataset.data.numpy().reshape(-1, 784) / 255.0\n    \n    return x_train, x_test","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Load data\nX_train, X_test = load_data()\n\n# 2. Initialize weights and biases (Students need to implement)\n\n# Example: W1 = np.random.randn(512, 784) * 0.01\n# Hint: Use np.zeros for biases\n\nW1 = np.random.randn(784, 512) * 0.01\nB1 = np.zeros(512)\n\nW2 = np.random.randn(512, 256) * 0.01 \nB2 = np.zeros(256) \n\nW3 = np.random.randn(256, 128) * 0.01\nB3 = np.zeros(128) \n\nW4 = np.random.randn(128, 256) * 0.01\nB4 = np.zeros(256)\n\nW5 = np.random.randn(256, 512) * 0.01\nB5 = np.zeros(512)\n\nW6 = np.random.randn(512, 784) * 0.01\nB6 = np.zeros(784)\n\n# 3. Define activation functions (Students need to implement)\ndef relu(x):\n    return np.max(0, x) \n\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x)) \n\n# 4. Implement forward pass (Encoder + Decoder) - Students need to implement\ndef forward(x):\n    # Encoder: multiple layers with ReLU activation\n    z1 = np.dot(W1, x) + B1\n    a1 = relu(z1)\n\n    z2 = np.dot(W2, a1) + B2\n    a2 = relu(z2)\n\n    z3 = np.dot(W3, a2) + B3\n    encoded = relu(z3)\n    \n    # Decoder: multiple layers with ReLU/Sigmoid activation for reconstruction\n    \n    return encoded.T, x_reconstructed.T\n\n# 5. Compute Mean Squared Error Loss (Students need to implement)\ndef mse_loss(x, x_reconstructed):\n    return np.mean((x - x_reconstructed)**2)\n\n# 6. Implement backpropagation and weight updates - Students need to implement\n# def backward(x, h1, h2, z, h3, h4, x_reconstructed, lr=0.001):\n#     # Compute gradients and update weights and biases\n#     pass\n\n# 7. Implement training loop - Students need to implement\n# def train(x_train, epochs=100, lr=0.001):\n#     # Train the network and collect loss and latent codes\n#     return losses, latent_codes, reconstructed_data\n\n# Train the model (Uncomment once students complete the train function)\n# losses, latent_codes, reconstructed_data = train(x_train)\n\n# 8. Plot the loss curve\ndef plot_loss_curve(losses):\n    plt.figure(figsize=(8, 6))\n    plt.plot(losses, label='MSE Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Curve')\n    plt.legend()\n    plt.show()\n\n# Uncomment after training to plot the loss curve\n# plot_loss_curve(losses)\n\n# 9. Visualize latent codes using PCA\ndef visualize_latent_space(latent_codes):\n    pca = PCA(n_components=2)\n    reduced_codes = pca.fit_transform(latent_codes)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(reduced_codes[:, 0], reduced_codes[:, 1], s=5, alpha=0.6)\n    plt.title('Latent Space Visualization (PCA)')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.show()\n\n# Uncomment after training to visualize latent space\n# visualize_latent_space(latent_codes)\n\n# 10. Show original and reconstructed images side by side\ndef show_images(original, reconstructed, n=10):\n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n    plt.show()\n\n# Uncomment after training to show images\n# show_images(x_train, reconstructed_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}