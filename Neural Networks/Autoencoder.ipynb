{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport os\n\nimport numpy as np\nfrom torchvision import datasets, transforms\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:10:08.215952Z","iopub.execute_input":"2024-11-12T17:10:08.217143Z","iopub.status.idle":"2024-11-12T17:10:14.767588Z","shell.execute_reply.started":"2024-11-12T17:10:08.217089Z","shell.execute_reply":"2024-11-12T17:10:14.766471Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def load_data():\n    transform = transforms.Compose([transforms.ToTensor(),\n                                    transforms.Lambda(lambda x: x.view(-1))])\n\n    train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n\n    x_train = train_dataset.data.numpy().reshape(-1, 784) / 255.0\n    x_test = test_dataset.data.numpy().reshape(-1, 784) / 255.0\n    \n    return x_train, x_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:11:24.379262Z","iopub.execute_input":"2024-11-12T17:11:24.379863Z","iopub.status.idle":"2024-11-12T17:11:24.387499Z","shell.execute_reply.started":"2024-11-12T17:11:24.379821Z","shell.execute_reply":"2024-11-12T17:11:24.386228Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def relu(x):\n    \"\"\"Applies the ReLU activation function element-wise.\n\n    Parameters:\n        x (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Array with ReLU applied, where each element is the maximum \n                    of 0 and the input element.\n    \"\"\"\n    return np.maximum(0, x)\n\ndef sigmoid(x):\n    \"\"\"Applies the Sigmoid activation function element-wise.\n\n    Parameters:\n        x (np.ndarray): Input array.\n\n    Returns:\n        np.ndarray: Array with Sigmoid applied to each element.\n    \"\"\"\n    return 1 / (1 + np.exp(-x))\n\ndef lin_combination(x, weights, bias):\n    \"\"\"Computes a linear combination of inputs, weights, and bias.\n\n    Parameters:\n        x (np.ndarray): Input array.\n        weights (np.ndarray): Weight matrix.\n        bias (np.ndarray): Bias vector.\n\n    Returns:\n        np.ndarray: Result of the linear combination.\n    \"\"\"\n    return np.dot(x, weights) + bias\n\ndef mse_loss(x, x_reconstructed):\n    return np.mean((x - x_reconstructed)**2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-12T17:51:01.422590Z","iopub.execute_input":"2024-11-12T17:51:01.423026Z","iopub.status.idle":"2024-11-12T17:51:01.470545Z","shell.execute_reply.started":"2024-11-12T17:51:01.422983Z","shell.execute_reply":"2024-11-12T17:51:01.469066Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 1. Load data\nX_train, X_test = load_data()\n\n# Initialize weights and biases with lowercase names\nw1 = np.random.randn(784, 512) * 0.01\nb1 = np.zeros(512)\n\nw2 = np.random.randn(512, 256) * 0.01 \nb2 = np.zeros(256) \n\nw3 = np.random.randn(256, 128) * 0.01\nb3 = np.zeros(128) \n\nw4 = np.random.randn(128, 256) * 0.01\nb4 = np.zeros(256)\n\nw5 = np.random.randn(256, 512) * 0.01\nb5 = np.zeros(512)\n\nw6 = np.random.randn(512, 784) * 0.01\nb6 = np.zeros(784)\n\ndef forward(x):\n    \"\"\"Performs a forward pass through the encoder and decoder network.\n\n    Encoder consists of multiple layers with ReLU activation, \n    and the decoder reconstructs the input using ReLU/Sigmoid activation.\n\n    Parameters:\n        x (np.ndarray): Input data.\n\n    Returns:\n        tuple: Encoded latent representation (z.T) and reconstructed input (x_reconstructed.T).\n    \"\"\"\n    # Encoder: multiple layers with ReLU activation\n    layer_1 = relu(lin_combination(x, w1, b1))\n    layer_2 = relu(lin_combination(layer_1, w2, b2))\n    layer_3 = relu(lin_combination(layer_2, w3, b3)) # Encoded\n\n    # Decoder: multiple layers with ReLU activation for reconstruction\n    layer_4 = relu(lin_combination(layer_3, w4, b4)) \n    layer_5 = relu(lin_combination(layer_4, w5, b5))\n    layer_6 = relu(lin_combination(layer_5, w6, b6)) # Decoded\n    \n    return layer_3.T, layer_6.T\n\n# 6. Implement backpropagation and weight updates - Students need to implement\ndef backward(x, h1, h2, z, h3, h4, x_reconstructed, lr=0.001):\n    # Compute gradients and update weights and biases\n    pass\n\n# 7. Implement training loop - Students need to implement\n# def train(x_train, epochs=100, lr=0.001):\n#     # Train the network and collect loss and latent codes\n#     return losses, latent_codes, reconstructed_data\n\n# Train the model (Uncomment once students complete the train function)\n# losses, latent_codes, reconstructed_data = train(x_train)\n\n# 8. Plot the loss curve\ndef plot_loss_curve(losses):\n    plt.figure(figsize=(8, 6))\n    plt.plot(losses, label='MSE Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Loss Curve')\n    plt.legend()\n    plt.show()\n\n# Uncomment after training to plot the loss curve\n# plot_loss_curve(losses)\n\n# 9. Visualize latent codes using PCA\ndef visualize_latent_space(latent_codes):\n    pca = PCA(n_components=2)\n    reduced_codes = pca.fit_transform(latent_codes)\n    plt.figure(figsize=(8, 6))\n    plt.scatter(reduced_codes[:, 0], reduced_codes[:, 1], s=5, alpha=0.6)\n    plt.title('Latent Space Visualization (PCA)')\n    plt.xlabel('Component 1')\n    plt.ylabel('Component 2')\n    plt.show()\n\n# Uncomment after training to visualize latent space\n# visualize_latent_space(latent_codes)\n\n# 10. Show original and reconstructed images side by side\ndef show_images(original, reconstructed, n=10):\n    plt.figure(figsize=(20, 4))\n    for i in range(n):\n        ax = plt.subplot(2, n, i + 1)\n        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n\n        ax = plt.subplot(2, n, i + 1 + n)\n        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n        plt.axis('off')\n    plt.show()\n\n# Uncomment after training to show images\n# show_images(x_train, reconstructed_data)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}