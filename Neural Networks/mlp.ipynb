{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T14:15:20.717667Z","iopub.execute_input":"2024-10-24T14:15:20.718235Z","iopub.status.idle":"2024-10-24T14:15:20.722473Z","shell.execute_reply.started":"2024-10-24T14:15:20.718182Z","shell.execute_reply":"2024-10-24T14:15:20.721383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport time\n\n\n# -------------------- Task 1: Generate a Complex Non-linear Dataset --------------------\ndef generate_complex_data(n_samples=5000):\n    \"\"\"\n    Task 1: Implement the function to generate a complex non-linear dataset.\n\n    Description:\n    Generate a dataset based on a non-linear function with added noise.\n    The function is defined as: y = 0.1x^3 - 0.5x^2 + 0.2x + 3 + sin(2x) + noise\n\n    Args:\n        n_samples: Number of samples to generate.\n\n    Returns:\n        X: Input feature array (n_samples,)\n        y: Target values (n_samples,)\n    \"\"\"\n    \n    # Generate X values\n    X = np.linspace(-10, 10, n_samples)\n    \n    noise = np.random.normal(0, 10, n_samples) # Generate noise randomly from gaussian distribution\n    \n    # Calculate y values with added noise\n    y = (0.1 * X**3) - (0.5 * X**2) + (0.2 * X + 3) + np.sin(2 * X) + noise\n    \n    return X, y\n\n# Prepare the dataset\nX, y = generate_complex_data()\nX = X.reshape(-1, 1)  # Reshape X to a 2D array\ny = y.reshape(-1, 1)  # Reshape y to a 2D array\n\nplt.plot(X, y)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T14:15:20.724042Z","iopub.execute_input":"2024-10-24T14:15:20.725037Z","iopub.status.idle":"2024-10-24T14:15:20.983876Z","shell.execute_reply.started":"2024-10-24T14:15:20.724979Z","shell.execute_reply":"2024-10-24T14:15:20.982910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Neural Network Class Definition --------------------\nclass TwoLayerMLP:\n    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n        \"\"\"\n        Task 4: Initialize the neural network parameters (weights and biases).\n\n        Description:\n        Define and initialize weights and biases for three layers:\n            1. Input to Hidden Layer 1\n            2. Hidden Layer 1 to Hidden Layer 2\n            3. Hidden Layer 2 to Output\n\n        Args:\n            input_size: Number of input features.\n            hidden1_size: Number of neurons in the first hidden layer.\n            hidden2_size: Number of neurons in the second hidden layer.\n            output_size: Number of output neurons.\n        \"\"\"\n        # TODO: Initialize weights and biases with appropriate dimensions\n        self.W1 = np.random.uniform(-0.01, 0.01, size=(input_size, 128)) \n        self.b1 = np.zeros(128) \n        \n        self.W2 = np.random.uniform(-0.01, 0.01, size=(128, 64)) \n        self.b2 = np.zeros(64)\n        \n        self.W3 = np.random.uniform(-0.01, 0.01, size=(64, 1))  \n        self.b3 = np.zeros(1)\n\n    def forward(self, X):\n        \"\"\"\n        Task 5: Implement the forward propagation logic.\n\n        Args:\n            X: Input features.\n\n        Returns:\n            output: Final output of the network.\n        \"\"\"\n        # TODO: Implement forward propagation logic for each layer\n        self.z1 = weighted_sum(X, self.W1, self.b1) # Replace with appropriate calculation\n        self.a1 = relu(self.z1) # Apply ReLU activation on z1\n\n        self.z2 = weighted_sum(X, self.W2, self.b2) # Replace with appropriate calculation\n        self.a2 = tanh(self.z2) # Apply Tanh activation on z2\n\n        self.z3 = weighted_sum(X, self.W3, self.b3) # Replace with appropriate calculation\n        output = self.z3 # No activation in the output layer for regression\n        \n        return output\n\n    def backward(self, X, y, output, lr=0.0005):\n        \"\"\"\n        Task 6: Implement backpropagation to calculate gradients and update weights.\n\n        Args:\n            X: Input features.\n            y: Ground truth target values.\n            output: Network predictions.\n            lr: Learning rate.\n        \"\"\"\n        # TODO: Calculate output layer error and backpropagate through the network\n        output_error = 0.5 * (y - output) ** 2  # Replace with appropriate calculation\n        \n        dW3 = None  # Replace with weight update calculation for W3\n        db3 = None  # Replace with bias update calculation for b3\n\n        a2_error = None  # Replace with error propagation for hidden layer 2\n        dW2 = None  # Replace with weight update calculation for W2\n        db2 = None  # Replace with bias update calculation for b2\n\n        a1_error = None  # Replace with error propagation for hidden layer 1\n        dW1 = None  # Replace with weight update calculation for W1\n        db1 = None  # Replace with bias update calculation for b1\n\n        # TODO: Update weights and biases using gradient descent\n        self.W3 -= lr * dW3\n        self.b3 -= lr * db3\n        self.W2 -= lr * dW2\n        self.b2 -= lr * db2\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n    def train(self, X, y, epochs=5000, lr=0.0005, batch_size=64):\n        \"\"\"\n        Task 7: Implement the training process using mini-batch gradient descent.\n\n        Args:\n            X: Input features.\n            y: Ground truth target values.\n            epochs: Number of training epochs.\n            lr: Learning rate.\n            batch_size: Size of each mini-batch.\n\n        Returns:\n            loss_history: List of training loss values over epochs.\n        \"\"\"\n        \n        loss_history = []\n        num_samples = len(X)\n        y_actual = actual_function(X)  #  TODO: Use actual function values for test loss calculation, define by yourself\n\n        start_time = time.time()  # Start measuring time\n\n        for epoch in range(epochs):\n            # Generate a randomly shuffled array\n            indices = np.random.permutation(len(X))\n            \n            X_shuffled = X[indices]\n            y_shuffled = y[indices]\n            \n            epoch_loss = 0 \n\n            # TODO: Divide the data into mini-batches and perform forward and backward propagation\n            for i in range(0, num_samples, batch_size):\n                X_batch = X_shuffled[batch_size]  # Extract mini-batch for X\n                y_batch = y_shuffled[batch_size]  # Extract mini-batch for y\n\n                output = self.forward(X_batch)  # Replace with forward pass for mini-batch\n                loss = mse_loss(y_batch, output)  # Replace with loss calculation for mini-batch\n                epoch_loss += loss \n                \n                # Perform backpropagation\n                self.backward(X_batch, y_batch, output, lr)\n\n            # Calculate and store loss for the entire training set\n            loss_history.append(epoch_loss)\n\n            # Calculate loss for the test set using actual function values\n            test_loss = None  # Replace with appropriate loss calculation using y_actual\n\n            # Print training and test loss every 100 epochs\n            if epoch % 100 == 0:\n                print(f'Epoch {epoch}, Training Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n            # Stop if training time exceeds 1 minute\n            if time.time() - start_time > 60:\n                print(f\"Stopping training early at epoch {epoch} due to time constraints.\")\n                break\n\n        return loss_history\n\n    def predict(self, X):\n        \"\"\"\n        Task 8: Implement the prediction logic using the trained model.\n\n        Args:\n            X: Input features.\n\n        Returns:\n            output: Predictions of the network.\n        \"\"\"\n        # TODO: Implement the prediction using the forward pass\n        return self.forward(X)","metadata":{"execution":{"iopub.status.busy":"2024-10-20T03:25:54.377927Z","iopub.execute_input":"2024-10-20T03:25:54.378710Z","iopub.status.idle":"2024-10-20T03:25:54.394869Z","shell.execute_reply.started":"2024-10-20T03:25:54.378669Z","shell.execute_reply":"2024-10-20T03:25:54.393464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Activation Functions and Their Derivatives --------------------\ndef relu(x):\n    \"\"\"\n    Task 2: Implement the ReLU activation function.\n    \"\"\"\n    # TODO: Implement ReLU activation function\n    \n    relu = max(0, x)\n    \n    return relu\n\n\ndef relu_derivative(x):\n    \"\"\"\n    Task 2: Implement the derivative of the ReLU activation function.\n    \"\"\"\n    # TODO: Implement ReLU derivative\n    \n    activation_output = relu(x)\n    \n    if activation_output == x: \n        return 1 \n    \n    return 0 \n\ndef tanh(x):\n    \"\"\"\n    Task 2: Implement the Tanh activation function.\n    \"\"\"\n    \n    return np.tanh(x)\n\n\ndef tanh_derivative(x):\n    \"\"\"\n    Task 2: Implement the derivative of the Tanh activation function.\n    \"\"\"\n\n    return 1 - (tanh(x) ** 2)\n\n\ndef weighted_sum(inputs, weights, bias): \n    \"\"\"\n    Computes the weighted sum of inputs, weights, and bias terms\n    \"\"\"\n    \n    return np.dot(inputs, weights) + bias ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:09:41.729184Z","iopub.execute_input":"2024-10-27T01:09:41.729657Z","iopub.status.idle":"2024-10-27T01:09:41.765626Z","shell.execute_reply.started":"2024-10-27T01:09:41.729610Z","shell.execute_reply":"2024-10-27T01:09:41.764427Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np \n\ninputs = np.array([0.5, 1.0, -0.5])            # Shape: (3,)\nweights = np.array([0.2, 0.8, -0.5])          # Shape: (3, 2) for 2 outputs)\nbias = np.zeros(3)                    # Shape: (2,)\n\noutput = weighted_sum(inputs, weights, bias)\noutput","metadata":{"execution":{"iopub.status.busy":"2024-10-27T01:26:13.496226Z","iopub.execute_input":"2024-10-27T01:26:13.496951Z","iopub.status.idle":"2024-10-27T01:26:13.510284Z","shell.execute_reply.started":"2024-10-27T01:26:13.496899Z","shell.execute_reply":"2024-10-27T01:26:13.508334Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"array([1.15, 1.15, 1.15])"},"metadata":{}}]},{"cell_type":"code","source":"# -------------------- Mean Squared Error Loss Function --------------------\ndef mse_loss(y_true, y_pred):\n    \"\"\"\n    Task 3: Implement the Mean Squared Error (MSE) loss function.\n\n    Args:\n        y_true: Ground truth values.\n        y_pred: Predicted values.\n\n    Returns:\n        loss: Computed mean squared error.\n    \"\"\"\n    \n    mse = np.mean((y_true - y_pred)**2)\n    \n    return mse ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Task 9: Data Preparation and Model Training --------------------\n\n# Prepare the dataset\nX, y = generate_complex_data()\nX = X.reshape(-1, 1)  # Reshape X to a 2D array\ny = y.reshape(-1, 1)  # Reshape y to a 2D array\n\n\n# # Create and train the model\n# model = TwoLayerMLP(input_size=1, hidden1_size=128, hidden2_size=64, output_size=1)\n# loss_history = model.train(X, y, epochs=5000, lr=0.0005, batch_size=64)\n\n# # -------------------- Task 10: Visualization of Training Results --------------------\n# # TODO: Plot the training loss over epochs using matplotlib\n# plt.plot(None)  # Replace with correct variable for loss history\n# plt.title(\"Training Loss Curve\")\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.show()\n\n# # -------------------- Task 11: Model Prediction and Visualization --------------------\n# # TODO: Use the trained model to predict and visualize results\n# y_pred = model.predict(None)  # Replace with appropriate prediction\n\n# # TODO: Plot the true data points and model predictions using matplotlib\n# plt.scatter(None, None, label='True Data', color='blue')  # Replace with true data\n# plt.plot(None, None, color='red', label='Predicted Data')  # Replace with predicted data\n# plt.title(\"True vs Predicted Data\")\n# plt.xlabel(\"X\")\n# plt.ylabel(\"y\")\n# plt.legend()\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}