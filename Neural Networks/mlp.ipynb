{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-17T02:46:18.888669Z","iopub.execute_input":"2024-10-17T02:46:18.889096Z","iopub.status.idle":"2024-10-17T02:46:20.453505Z","shell.execute_reply.started":"2024-10-17T02:46:18.889049Z","shell.execute_reply":"2024-10-17T02:46:20.451967Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n\n# -------------------- Task 1: Generate a Complex Non-linear Dataset --------------------\ndef generate_complex_data(n_samples=5000):\n    \"\"\"\n    Task 1: Implement the function to generate a complex non-linear dataset.\n\n    Description:\n    Generate a dataset based on a non-linear function with added noise.\n    The function is defined as: y = 0.1x^3 - 0.5x^2 + 0.2x + 3 + sin(2x) + noise\n\n    Args:\n        n_samples: Number of samples to generate.\n\n    Returns:\n        X: Input feature array (n_samples,)\n        y: Target values (n_samples,)\n    \"\"\"\n    # TODO: Implement the dataset generation logic using the defined complex function\n    X = None  # Replace with appropriate X values generation\n    y = None  # Replace with appropriate y values generation\n    return X, y","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Neural Network Class Definition --------------------\nclass TwoLayerMLP:\n    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n        \"\"\"\n        Task 4: Initialize the neural network parameters (weights and biases).\n\n        Description:\n        Define and initialize weights and biases for three layers:\n            1. Input to Hidden Layer 1\n            2. Hidden Layer 1 to Hidden Layer 2\n            3. Hidden Layer 2 to Output\n\n        Args:\n            input_size: Number of input features.\n            hidden1_size: Number of neurons in the first hidden layer.\n            hidden2_size: Number of neurons in the second hidden layer.\n            output_size: Number of output neurons.\n        \"\"\"\n        # TODO: Initialize weights and biases with appropriate dimensions\n        self.W1 = None  # Replace with initialization\n        self.b1 = None  # Replace with initialization\n        self.W2 = None  # Replace with initialization\n        self.b2 = None  # Replace with initialization\n        self.W3 = None  # Replace with initialization\n        self.b3 = None  # Replace with initialization\n\n    def forward(self, X):\n        \"\"\"\n        Task 5: Implement the forward propagation logic.\n\n        Args:\n            X: Input features.\n\n        Returns:\n            output: Final output of the network.\n        \"\"\"\n        # TODO: Implement forward propagation logic for each layer\n        self.z1 = None  # Replace with appropriate calculation\n        self.a1 = None  # Apply ReLU activation on z1\n\n        self.z2 = None  # Replace with appropriate calculation\n        self.a2 = None  # Apply Tanh activation on z2\n\n        self.z3 = None  # Replace with appropriate calculation\n        output = None  # No activation in the output layer for regression\n        return output\n\n    def backward(self, X, y, output, lr=0.0005):\n        \"\"\"\n        Task 6: Implement backpropagation to calculate gradients and update weights.\n\n        Args:\n            X: Input features.\n            y: Ground truth target values.\n            output: Network predictions.\n            lr: Learning rate.\n        \"\"\"\n        # TODO: Calculate output layer error and backpropagate through the network\n        output_error = None  # Replace with appropriate calculation\n\n        dW3 = None  # Replace with weight update calculation for W3\n        db3 = None  # Replace with bias update calculation for b3\n\n        a2_error = None  # Replace with error propagation for hidden layer 2\n        dW2 = None  # Replace with weight update calculation for W2\n        db2 = None  # Replace with bias update calculation for b2\n\n        a1_error = None  # Replace with error propagation for hidden layer 1\n        dW1 = None  # Replace with weight update calculation for W1\n        db1 = None  # Replace with bias update calculation for b1\n\n        # TODO: Update weights and biases using gradient descent\n        self.W3 -= lr * dW3\n        self.b3 -= lr * db3\n        self.W2 -= lr * dW2\n        self.b2 -= lr * db2\n        self.W1 -= lr * dW1\n        self.b1 -= lr * db1\n\n    def train(self, X, y, epochs=5000, lr=0.0005, batch_size=64):\n        \"\"\"\n        Task 7: Implement the training process using mini-batch gradient descent.\n\n        Args:\n            X: Input features.\n            y: Ground truth target values.\n            epochs: Number of training epochs.\n            lr: Learning rate.\n            batch_size: Size of each mini-batch.\n\n        Returns:\n            loss_history: List of training loss values over epochs.\n        \"\"\"\n        loss_history = []\n        num_samples = len(X)\n        y_actual = actual_function(X)  #  TODO: Use actual function values for test loss calculation, define by yourself\n\n        start_time = time.time()  # Start measuring time\n\n        for epoch in range(epochs):\n            # TODO: Shuffle the dataset at the start of each epoch\n            indices = None  # Replace with appropriate shuffling logic\n            X_shuffled = None  # Replace with shuffled X\n            y_shuffled = None  # Replace with shuffled y\n\n            # TODO: Divide the data into mini-batches and perform forward and backward propagation\n            for i in range(0, num_samples, batch_size):\n                X_batch = None  # Extract mini-batch for X\n                y_batch = None  # Extract mini-batch for y\n\n                output = None  # Replace with forward pass for mini-batch\n                loss = None  # Replace with loss calculation for mini-batch\n\n                # Perform backpropagation\n                self.backward(X_batch, y_batch, output, lr)\n\n            # Calculate and store loss for the entire training set\n            epoch_loss = None  # Replace with appropriate loss calculation\n            loss_history.append(epoch_loss)\n\n            # Calculate loss for the test set using actual function values\n            test_loss = None  # Replace with appropriate loss calculation using y_actual\n\n            # Print training and test loss every 100 epochs\n            if epoch % 100 == 0:\n                print(f'Epoch {epoch}, Training Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}')\n\n            # Stop if training time exceeds 1 minute\n            if time.time() - start_time > 60:\n                print(f\"Stopping training early at epoch {epoch} due to time constraints.\")\n                break\n\n        return loss_history\n\n    def predict(self, X):\n        \"\"\"\n        Task 8: Implement the prediction logic using the trained model.\n\n        Args:\n            X: Input features.\n\n        Returns:\n            output: Predictions of the network.\n        \"\"\"\n        # TODO: Implement the prediction using the forward pass\n        return None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Activation Functions and Their Derivatives --------------------\ndef relu(x):\n    \"\"\"\n    Task 2: Implement the ReLU activation function.\n    \"\"\"\n    # TODO: Implement ReLU activation function\n    \n    relu = max(0, x)\n    \n    return relu\n\n\ndef relu_derivative(x):\n    \"\"\"\n    Task 2: Implement the derivative of the ReLU activation function.\n    \"\"\"\n    # TODO: Implement ReLU derivative\n    \n    activation_output = relu(x)\n    \n    if activation_output == x: \n        return 1 \n    \n    return 0 \n\ndef tanh(x):\n    \"\"\"\n    Task 2: Implement the Tanh activation function.\n    \"\"\"\n    # TODO: Implement Tanh activation function\n    return None\n\n\ndef tanh_derivative(x):\n    \"\"\"\n    Task 2: Implement the derivative of the Tanh activation function.\n    \"\"\"\n    # TODO: Implement Tanh derivative\n    return None","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Mean Squared Error Loss Function --------------------\ndef mse_loss(y_true, y_pred):\n    \"\"\"\n    Task 3: Implement the Mean Squared Error (MSE) loss function.\n\n    Args:\n        y_true: Ground truth values.\n        y_pred: Predicted values.\n\n    Returns:\n        loss: Computed mean squared error.\n    \"\"\"\n    \n    mse = np.mean((y_true - y_pred)**2)\n    \n    return mse ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -------------------- Task 9: Data Preparation and Model Training --------------------\n# Prepare the dataset\n# X, y = generate_complex_data()\n# X = X.reshape(-1, 1)  # Reshape X to a 2D array\n# y = y.reshape(-1, 1)  # Reshape y to a 2D array\n\n# # Create and train the model\n# model = TwoLayerMLP(input_size=1, hidden1_size=128, hidden2_size=64, output_size=1)\n# loss_history = model.train(X, y, epochs=5000, lr=0.0005, batch_size=64)\n\n# # -------------------- Task 10: Visualization of Training Results --------------------\n# # TODO: Plot the training loss over epochs using matplotlib\n# plt.plot(None)  # Replace with correct variable for loss history\n# plt.title(\"Training Loss Curve\")\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.show()\n\n# # -------------------- Task 11: Model Prediction and Visualization --------------------\n# # TODO: Use the trained model to predict and visualize results\n# y_pred = model.predict(None)  # Replace with appropriate prediction\n\n# # TODO: Plot the true data points and model predictions using matplotlib\n# plt.scatter(None, None, label='True Data', color='blue')  # Replace with true data\n# plt.plot(None, None, color='red', label='Predicted Data')  # Replace with predicted data\n# plt.title(\"True vs Predicted Data\")\n# plt.xlabel(\"X\")\n# plt.ylabel(\"y\")\n# plt.legend()\n# plt.show()","metadata":{},"execution_count":null,"outputs":[]}]}