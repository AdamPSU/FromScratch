{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e127a130",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:55.681606Z",
     "iopub.status.busy": "2024-10-22T02:52:55.681171Z",
     "iopub.status.idle": "2024-10-22T02:52:56.668306Z",
     "shell.execute_reply": "2024-10-22T02:52:56.666758Z"
    },
    "papermill": {
     "duration": 0.995815,
     "end_time": "2024-10-22T02:52:56.671468",
     "exception": false,
     "start_time": "2024-10-22T02:52:55.675653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f19ab55",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:56.681454Z",
     "iopub.status.busy": "2024-10-22T02:52:56.680560Z",
     "iopub.status.idle": "2024-10-22T02:52:58.563426Z",
     "shell.execute_reply": "2024-10-22T02:52:58.562162Z"
    },
    "papermill": {
     "duration": 1.890957,
     "end_time": "2024-10-22T02:52:58.566294",
     "exception": false,
     "start_time": "2024-10-22T02:52:56.675337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import time\n",
    "\n",
    "\n",
    "# -------------------- Task 1: Generate a Complex Non-linear Dataset --------------------\n",
    "def generate_complex_data(n_samples=5000):\n",
    "    \"\"\"\n",
    "    Task 1: Implement the function to generate a complex non-linear dataset.\n",
    "\n",
    "    Description:\n",
    "    Generate a dataset based on a non-linear function with added noise.\n",
    "    The function is defined as: y = 0.1x^3 - 0.5x^2 + 0.2x + 3 + sin(2x) + noise\n",
    "\n",
    "    Args:\n",
    "        n_samples: Number of samples to generate.\n",
    "\n",
    "    Returns:\n",
    "        X: Input feature array (n_samples,)\n",
    "        y: Target values (n_samples,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate X values\n",
    "    X = np.linspace(-10, 10, n_samples)\n",
    "    \n",
    "    noise = np.random.normal(0, 10, n_samples) # Generate noise randomly from gaussian distribution\n",
    "    \n",
    "    # Calculate y values with added noise\n",
    "    y = (0.1 * X**3) - (0.5 * X**2) + (0.2 * X + 3) + np.sin(2 * X) + noise\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Prepare the dataset\n",
    "# X, y = generate_complex_data()\n",
    "# X = X.reshape(-1, 1)  # Reshape X to a 2D array\n",
    "# y = y.reshape(-1, 1)  # Reshape y to a 2D array\n",
    "\n",
    "# plt.plot(X, y)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3df2223",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:58.575286Z",
     "iopub.status.busy": "2024-10-22T02:52:58.574700Z",
     "iopub.status.idle": "2024-10-22T02:52:58.597624Z",
     "shell.execute_reply": "2024-10-22T02:52:58.596439Z"
    },
    "papermill": {
     "duration": 0.030713,
     "end_time": "2024-10-22T02:52:58.600402",
     "exception": false,
     "start_time": "2024-10-22T02:52:58.569689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------- Neural Network Class Definition --------------------\n",
    "class TwoLayerMLP:\n",
    "    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):\n",
    "        \"\"\"\n",
    "        Task 4: Initialize the neural network parameters (weights and biases).\n",
    "\n",
    "        Description:\n",
    "        Define and initialize weights and biases for three layers:\n",
    "            1. Input to Hidden Layer 1\n",
    "            2. Hidden Layer 1 to Hidden Layer 2\n",
    "            3. Hidden Layer 2 to Output\n",
    "\n",
    "        Args:\n",
    "            input_size: Number of input features.\n",
    "            hidden1_size: Number of neurons in the first hidden layer.\n",
    "            hidden2_size: Number of neurons in the second hidden layer.\n",
    "            output_size: Number of output neurons.\n",
    "        \"\"\"\n",
    "        # TODO: Initialize weights and biases with appropriate dimensions\n",
    "        self.W1 = np.random.uniform(-0.01, 0.01, size=128) \n",
    "        self.b1 = 0  # Replace with initialization\n",
    "        self.W2 = np.random.uniform(-0.01, 0.01, size=64) \n",
    "        self.b2 = 0  # Replace with initialization\n",
    "        self.W3 = np.random.uniform(-0.01, 0.01, size=1)  \n",
    "        self.b3 = 0  # Replace with initialization\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Task 5: Implement the forward propagation logic.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "\n",
    "        Returns:\n",
    "            output: Final output of the network.\n",
    "        \"\"\"\n",
    "        # TODO: Implement forward propagation logic for each layer\n",
    "        self.z1 = weighted_sum(X, self.W1, self.b1) # Replace with appropriate calculation\n",
    "        self.a1 = relu(self.z1) # Apply ReLU activation on z1\n",
    "\n",
    "        self.z2 = weighted_sum(X, self.W2, self.b2) # Replace with appropriate calculation\n",
    "        self.a2 = tanh(self.z2) # Apply Tanh activation on z2\n",
    "\n",
    "        self.z3 = weighted_sum(X, self.W3, self.b3) # Replace with appropriate calculation\n",
    "        output = self.z3 # No activation in the output layer for regression\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, X, y, output, lr=0.0005):\n",
    "        \"\"\"\n",
    "        Task 6: Implement backpropagation to calculate gradients and update weights.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "            y: Ground truth target values.\n",
    "            output: Network predictions.\n",
    "            lr: Learning rate.\n",
    "        \"\"\"\n",
    "        # TODO: Calculate output layer error and backpropagate through the network\n",
    "        output_error = mse_loss(y, output)  # Replace with appropriate calculation\n",
    "\n",
    "        dW3 = None  # Replace with weight update calculation for W3\n",
    "        db3 = None  # Replace with bias update calculation for b3\n",
    "\n",
    "        a2_error = None  # Replace with error propagation for hidden layer 2\n",
    "        dW2 = None  # Replace with weight update calculation for W2\n",
    "        db2 = None  # Replace with bias update calculation for b2\n",
    "\n",
    "        a1_error = None  # Replace with error propagation for hidden layer 1\n",
    "        dW1 = None  # Replace with weight update calculation for W1\n",
    "        db1 = None  # Replace with bias update calculation for b1\n",
    "\n",
    "        # TODO: Update weights and biases using gradient descent\n",
    "        self.W3 -= lr * dW3\n",
    "        self.b3 -= lr * db3\n",
    "        self.W2 -= lr * dW2\n",
    "        self.b2 -= lr * db2\n",
    "        self.W1 -= lr * dW1\n",
    "        self.b1 -= lr * db1\n",
    "\n",
    "    def train(self, X, y, epochs=5000, lr=0.0005, batch_size=64):\n",
    "        \"\"\"\n",
    "        Task 7: Implement the training process using mini-batch gradient descent.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "            y: Ground truth target values.\n",
    "            epochs: Number of training epochs.\n",
    "            lr: Learning rate.\n",
    "            batch_size: Size of each mini-batch.\n",
    "\n",
    "        Returns:\n",
    "            loss_history: List of training loss values over epochs.\n",
    "        \"\"\"\n",
    "        \n",
    "        loss_history = []\n",
    "        num_samples = len(X)\n",
    "        y_actual = actual_function(X)  #  TODO: Use actual function values for test loss calculation, define by yourself\n",
    "\n",
    "        start_time = time.time()  # Start measuring time\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Generate a randomly shuffled array\n",
    "            indices = np.random.permutation(len(X))\n",
    "            \n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            # TODO: Divide the data into mini-batches and perform forward and backward propagation\n",
    "            for i in range(0, num_samples, batch_size):\n",
    "                X_batch = X_shuffled[batch_size]  # Extract mini-batch for X\n",
    "                y_batch = y_shuffled[batch_size]  # Extract mini-batch for y\n",
    "\n",
    "                output = self.forward(X_batch)  # Replace with forward pass for mini-batch\n",
    "                loss = None  # Replace with loss calculation for mini-batch\n",
    "\n",
    "                # Perform backpropagation\n",
    "                self.backward(X_batch, y_batch, output, lr)\n",
    "\n",
    "            # Calculate and store loss for the entire training set\n",
    "            epoch_loss = None  # Replace with appropriate loss calculation\n",
    "            loss_history.append(epoch_loss)\n",
    "\n",
    "            # Calculate loss for the test set using actual function values\n",
    "            test_loss = None  # Replace with appropriate loss calculation using y_actual\n",
    "\n",
    "            # Print training and test loss every 100 epochs\n",
    "            if epoch % 100 == 0:\n",
    "                print(f'Epoch {epoch}, Training Loss: {epoch_loss:.4f}, Test Loss: {test_loss:.4f}')\n",
    "\n",
    "            # Stop if training time exceeds 1 minute\n",
    "            if time.time() - start_time > 60:\n",
    "                print(f\"Stopping training early at epoch {epoch} due to time constraints.\")\n",
    "                break\n",
    "\n",
    "        return loss_history\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Task 8: Implement the prediction logic using the trained model.\n",
    "\n",
    "        Args:\n",
    "            X: Input features.\n",
    "\n",
    "        Returns:\n",
    "            output: Predictions of the network.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the prediction using the forward pass\n",
    "        return self.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eb17fdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:58.609221Z",
     "iopub.status.busy": "2024-10-22T02:52:58.608759Z",
     "iopub.status.idle": "2024-10-22T02:52:58.617534Z",
     "shell.execute_reply": "2024-10-22T02:52:58.616393Z"
    },
    "papermill": {
     "duration": 0.015976,
     "end_time": "2024-10-22T02:52:58.619986",
     "exception": false,
     "start_time": "2024-10-22T02:52:58.604010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------- Activation Functions and Their Derivatives --------------------\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Task 2: Implement the ReLU activation function.\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU activation function\n",
    "    \n",
    "    relu = max(0, x)\n",
    "    \n",
    "    return relu\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Task 2: Implement the derivative of the ReLU activation function.\n",
    "    \"\"\"\n",
    "    # TODO: Implement ReLU derivative\n",
    "    \n",
    "    activation_output = relu(x)\n",
    "    \n",
    "    if activation_output == x: \n",
    "        return 1 \n",
    "    \n",
    "    return 0 \n",
    "\n",
    "def tanh(x):\n",
    "    \"\"\"\n",
    "    Task 2: Implement the Tanh activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    \"\"\"\n",
    "    Task 2: Implement the derivative of the Tanh activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    return 1 - (tanh(x) ** 2)\n",
    "\n",
    "\n",
    "def weighted_sum(inputs, weights, bias): \n",
    "    \"\"\"\n",
    "    Computes the weighted sum of inputs, weights, and bias terms\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.dot(inputs, weights) + bias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "586eb090",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:58.628594Z",
     "iopub.status.busy": "2024-10-22T02:52:58.628191Z",
     "iopub.status.idle": "2024-10-22T02:52:58.635202Z",
     "shell.execute_reply": "2024-10-22T02:52:58.633919Z"
    },
    "papermill": {
     "duration": 0.014434,
     "end_time": "2024-10-22T02:52:58.637906",
     "exception": false,
     "start_time": "2024-10-22T02:52:58.623472",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------- Mean Squared Error Loss Function --------------------\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Task 3: Implement the Mean Squared Error (MSE) loss function.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground truth values.\n",
    "        y_pred: Predicted values.\n",
    "\n",
    "    Returns:\n",
    "        loss: Computed mean squared error.\n",
    "    \"\"\"\n",
    "    \n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    \n",
    "    return mse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d263c616",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-22T02:52:58.647491Z",
     "iopub.status.busy": "2024-10-22T02:52:58.647028Z",
     "iopub.status.idle": "2024-10-22T02:52:58.653826Z",
     "shell.execute_reply": "2024-10-22T02:52:58.652630Z"
    },
    "papermill": {
     "duration": 0.014479,
     "end_time": "2024-10-22T02:52:58.656459",
     "exception": false,
     "start_time": "2024-10-22T02:52:58.641980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------- Task 9: Data Preparation and Model Training --------------------\n",
    "\n",
    "# # Prepare the dataset\n",
    "# X, y = generate_complex_data()\n",
    "# X = X.reshape(-1, 1)  # Reshape X to a 2D array\n",
    "# y = y.reshape(-1, 1)  # Reshape y to a 2D array\n",
    "\n",
    "\n",
    "# # Create and train the model\n",
    "# model = TwoLayerMLP(input_size=1, hidden1_size=128, hidden2_size=64, output_size=1)\n",
    "# loss_history = model.train(X, y, epochs=5000, lr=0.0005, batch_size=64)\n",
    "\n",
    "# # -------------------- Task 10: Visualization of Training Results --------------------\n",
    "# # TODO: Plot the training loss over epochs using matplotlib\n",
    "# plt.plot(None)  # Replace with correct variable for loss history\n",
    "# plt.title(\"Training Loss Curve\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.show()\n",
    "\n",
    "# # -------------------- Task 11: Model Prediction and Visualization --------------------\n",
    "# # TODO: Use the trained model to predict and visualize results\n",
    "# y_pred = model.predict(None)  # Replace with appropriate prediction\n",
    "\n",
    "# # TODO: Plot the true data points and model predictions using matplotlib\n",
    "# plt.scatter(None, None, label='True Data', color='blue')  # Replace with true data\n",
    "# plt.plot(None, None, color='red', label='Predicted Data')  # Replace with predicted data\n",
    "# plt.title(\"True vs Predicted Data\")\n",
    "# plt.xlabel(\"X\")\n",
    "# plt.ylabel(\"y\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.715162,
   "end_time": "2024-10-22T02:52:59.282406",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-22T02:52:52.567244",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
