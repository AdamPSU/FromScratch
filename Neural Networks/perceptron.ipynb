{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65711,"databundleVersionId":7405009,"sourceType":"competition"}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T00:07:48.708927Z","iopub.execute_input":"2024-09-28T00:07:48.709476Z","iopub.status.idle":"2024-09-28T00:07:49.151005Z","shell.execute_reply.started":"2024-09-28T00:07:48.709412Z","shell.execute_reply":"2024-09-28T00:07:49.149732Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def _step_function(weighted_sum, threshold=0): \n    \"\"\"\n    Implements the signum activation function\n    for a given weighted sum. An activation function\n    aims to introduce non-linearity to a system, enabling\n    the system to capture more complex patterns. \n    \n    The signum activation function normalizes the output layer\n    into one of three values: -1, 0, or 1. \n    \"\"\"\n    \n    if weighted_sum > threshold:\n        return 1 \n    \n    elif weighted_sum < -threshold: \n        return -1\n    \n    return 0\n\ndef _weighted_sum(feature_space, weight_vector, bias):\n    \"\"\"\n    Helper function to calculate the matrix-vector \n    multiplication between the feature space and the \n    weight vector, plus a bias. \n    \"\"\"\n\n    weighted_sum = np.dot(feature_space, weight_vector) + bias\n\n    return weighted_sum ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:14:38.265357Z","iopub.execute_input":"2024-09-28T01:14:38.265930Z","iopub.status.idle":"2024-09-28T01:14:38.274337Z","shell.execute_reply.started":"2024-09-28T01:14:38.265879Z","shell.execute_reply":"2024-09-28T01:14:38.272981Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"class Perceptron:\n    # TODO: Convert all pandas to base Python\n    \n    def __init__(self, learning_rate=0.3, epochs=100, threshold=0):\n        self.learning_rate = learning_rate\n        self.epochs = epochs \n        self.threshold = threshold \n\n        self.weights = None \n        self.bias = None \n\n    def predict(self, X):\n        weighted_sum = _weighted_sum(X, self.weights, self.bias)\n        activated_sum = _step_function(weighted_sum, self.threshold)\n        \n        return activated_sum\n\n    def fit(self, X, y):\n        input_columns = X.shape[1]\n\n        self.weights = np.random.uniform(-0.01, 0.01, size=input_columns)\n        self.bias = 0\n                \n        for epoch in range(self.epochs): \n            error_sum = 0 \n            \n            for i in range(input_columns): \n                x_i, y_i = X[i], y[i]\n                y_pred = self.predict(x_i) \n\n                error = y_i - y_pred\n                error_sum += error**2\n                \n                if error != 0: \n                    self.weights += self.learning_rate * y_i * x_i\n                    self.bias += self.learning_rate * y_i\n                    \n            print(self.weights, self.bias)\n                    \n            if error_sum == 0: \n                break ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T01:47:16.349658Z","iopub.execute_input":"2024-09-28T01:47:16.350131Z","iopub.status.idle":"2024-09-28T01:47:16.362687Z","shell.execute_reply.started":"2024-09-28T01:47:16.350087Z","shell.execute_reply":"2024-09-28T01:47:16.361599Z"},"trusted":true},"execution_count":74,"outputs":[]}]}